{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chunsj1020/line-tracer/blob/master/Copy_of_train_detect_own_dataset_tf_colab4_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOpn4IebMl6p"
      },
      "source": [
        "# 필요 패키지 설치\n",
        "- Google colab 설치 및 실행\n",
        "- Object detection TF1 기준\n",
        "- Dataset [Kaggle fruit dataset](https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkvLlklwNhg_",
        "outputId": "6b4d8126-16b0-439c-fca0-1ce8e791c062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_gpu==1.15\n",
            "  Downloading tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5 MB 6.6 kB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (1.13.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (0.37.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (3.17.3)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 48.0 MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (1.42.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (0.2.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 90.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow_gpu==1.15) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (3.3.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow_gpu==1.15) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=a7010c7adcad094b87e14a98eb77bd6abb2641c0c6e6e8a2efb88c486ebefa0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow-gpu\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 1.15.0 which is incompatible.\n",
            "tensorflow 2.7.0 requires tensorflow-estimator<2.8,~=2.7.0rc0, but you have tensorflow-estimator 1.15.1 which is incompatible.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"
          ]
        }
      ],
      "source": [
        "# Colab GPU 할당 주의\n",
        "!pip install tensorflow_gpu==1.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTGqlSTLuxld",
        "outputId": "211ec20b-4dc7-4543-c921-e2e565765298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "protobuf-compiler is already the newest version (3.0.0-9.1ubuntu1).\n",
            "python-tk is already the newest version (2.7.17-1~18.04).\n",
            "The following additional packages will be installed:\n",
            "  python-bs4 python-chardet python-html5lib python-olefile\n",
            "  python-pkg-resources python-six python-webencodings\n",
            "Suggested packages:\n",
            "  python-genshi python-lxml-dbg python-lxml-doc python-pil-doc python-pil-dbg\n",
            "  python-setuptools\n",
            "The following NEW packages will be installed:\n",
            "  python-bs4 python-chardet python-html5lib python-lxml python-olefile\n",
            "  python-pil python-pkg-resources python-six python-webencodings\n",
            "0 upgraded, 9 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 1,614 kB of archives.\n",
            "After this operation, 8,908 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-bs4 all 4.6.0-1 [67.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-pkg-resources all 39.0.1-2 [128 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-chardet all 3.0.4-1 [80.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-six all 1.11.0-2 [11.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-webencodings all 0.5-2 [10.3 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-html5lib all 0.999999999-1 [83.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python-lxml amd64 4.2.1-1ubuntu0.4 [897 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-olefile all 0.45.1-1 [33.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python-pil amd64 5.1.0-1ubuntu0.6 [302 kB]\n",
            "Fetched 1,614 kB in 3s (591 kB/s)\n",
            "Selecting previously unselected package python-bs4.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n",
            "Unpacking python-bs4 (4.6.0-1) ...\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python-chardet.\n",
            "Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n",
            "Unpacking python-chardet (3.0.4-1) ...\n",
            "Selecting previously unselected package python-six.\n",
            "Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n",
            "Unpacking python-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python-webencodings.\n",
            "Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n",
            "Unpacking python-webencodings (0.5-2) ...\n",
            "Selecting previously unselected package python-html5lib.\n",
            "Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n",
            "Unpacking python-html5lib (0.999999999-1) ...\n",
            "Selecting previously unselected package python-lxml:amd64.\n",
            "Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.4_amd64.deb ...\n",
            "Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.4) ...\n",
            "Selecting previously unselected package python-olefile.\n",
            "Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n",
            "Unpacking python-olefile (0.45.1-1) ...\n",
            "Selecting previously unselected package python-pil:amd64.\n",
            "Preparing to unpack .../8-python-pil_5.1.0-1ubuntu0.6_amd64.deb ...\n",
            "Unpacking python-pil:amd64 (5.1.0-1ubuntu0.6) ...\n",
            "Setting up python-pkg-resources (39.0.1-2) ...\n",
            "Setting up python-six (1.11.0-2) ...\n",
            "Setting up python-bs4 (4.6.0-1) ...\n",
            "Setting up python-lxml:amd64 (4.2.1-1ubuntu0.4) ...\n",
            "Setting up python-olefile (0.45.1-1) ...\n",
            "Setting up python-pil:amd64 (5.1.0-1ubuntu0.6) ...\n",
            "Setting up python-webencodings (0.5-2) ...\n",
            "Setting up python-chardet (3.0.4-1) ...\n",
            "Setting up python-html5lib (0.999999999-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (0.29.24)\n",
            "Collecting tf_slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf_slim) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n",
            "Installing collected packages: tf-slim\n",
            "Successfully installed tf-slim-1.1.0\n"
          ]
        }
      ],
      "source": [
        "# 필요 패키지 설치\n",
        "!apt-get install protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install Cython tf_slim\n",
        "!pip install -q pycocotools\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib\n",
        "#!git clone https://github.com/tensorflow/models.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us0ie7745Ja3"
      },
      "outputs": [],
      "source": [
        "# 기존 models가 만약 존재하면 삭제 후 다시 clone 수행\n",
        "!rm -rf models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaIFd76k4-NA",
        "outputId": "b0472892-49ef-4c03-dacf-6882a3e7bca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 21980, done.\u001b[K\n",
            "remote: Total 21980 (delta 0), reused 0 (delta 0), pack-reused 21980\u001b[K\n",
            "Receiving objects: 100% (21980/21980), 520.67 MiB | 13.77 MiB/s, done.\n",
            "Resolving deltas: 100% (13093/13093), done.\n",
            "Checking out files: 100% (2768/2768), done.\n"
          ]
        }
      ],
      "source": [
        "# TF1 사용 - 1.13.0 branch clone- https://github.com/tensorflow/models/tree/r1.13.0\n",
        "!git clone https://github.com/tensorflow/models --branch r1.13.0 --single-branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_l3Linl48OF",
        "outputId": "eb665455-3539-492f-c650-9c391355031f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research\n",
            "env: PYTHONPATH=/content/models/research:/content/models/research/slim\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/slim/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/models/research/slim/nets/mobilenet/mobilenet.py:389: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Running tests under Python 3.7.12: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTest.test_create_embedded_ssd_mobilenet_v1_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_embedded_ssd_mobilenet_v1_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_inception_resnet_v2_model_from_config\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/anchor_generators/grid_anchor_generator.py:59: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0103 03:29:48.181557 140341753374592 deprecation.py:323] From /content/models/research/object_detection/anchor_generators/grid_anchor_generator.py:59: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_inception_resnet_v2_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_inception_v2_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_inception_v2_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_nas_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_nas_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_pnas_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_pnas_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_resnet101_with_mask_prediction_enabled0 (use_matmul_crop_and_resize=False)\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_resnet101_with_mask_prediction_enabled0 (use_matmul_crop_and_resize=False)\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_resnet101_with_mask_prediction_enabled1 (use_matmul_crop_and_resize=True)\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_resnet101_with_mask_prediction_enabled1 (use_matmul_crop_and_resize=True)\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_resnet_v1_models_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_resnet_v1_models_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_rfcn_resnet_v1_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_rfcn_resnet_v1_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_inception_v2_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_inception_v2_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_inception_v3_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_inception_v3_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_mobilenet_v1_fpn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_mobilenet_v1_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_mobilenet_v1_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_mobilenet_v1_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_mobilenet_v1_ppn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_mobilenet_v1_ppn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_mobilenet_v2_fpn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_mobilenet_v2_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_mobilenet_v2_fpnlite_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_mobilenet_v2_fpnlite_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_mobilenet_v2_keras_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_mobilenet_v2_keras_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_mobilenet_v2_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_mobilenet_v2_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_resnet_v1_fpn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_resnet_v1_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_resnet_v1_ppn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_resnet_v1_ppn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_session\n",
            "[  SKIPPED ] ModelBuilderTest.test_session\n",
            "----------------------------------------------------------------------\n",
            "Ran 22 tests in 0.089s\n",
            "\n",
            "OK (skipped=1)\n"
          ]
        }
      ],
      "source": [
        "# Object detection 필요 설치\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "%set_env PYTHONPATH=/content/models/research:/content/models/research/slim\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/models\"\n",
        "import sys\n",
        "sys.path.append(\"/content/models\")\n",
        "\n",
        "!python object_detection/builders/model_builder_test.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiXiLQumY-nz"
      },
      "source": [
        "# 데이터셋 다운로드 / 압축 해제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqM7xydXIZ2X"
      },
      "outputs": [],
      "source": [
        "#%rm -rf /content/dataset/  # 기존 데이터셋이 만약 존재하면 삭제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-R8Np0LHaA6",
        "outputId": "81311e0c-c17b-4881-b1b1-c2a9ef3eaaf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/dataset\n"
          ]
        }
      ],
      "source": [
        "# 작업 path를 /content/dataset 으로 설정\n",
        "%cd /content\n",
        "%mkdir dataset\n",
        "%cd /content/dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2CRNGhbN_XbI",
        "outputId": "fbdf7a79-89da-4a85-cdb8-e3c43f105351",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/dataset'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "Kp4UnFlF_-kI",
        "outputId": "2648ae7c-d4b7-42e2-f17e-a4f5a7975b08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3rDsdh4Kjci",
        "outputId": "cfe932cd-6cd0-4ff5-ef53-ec87ba4cab69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-03 03:42:05--  https://dwstor01.blob.core.windows.net/snue/fruit_dataset.zip?sp=r&st=2021-12-29T02:54:03Z&s[%E2%80%A6]mRXtTvX6mydEyb9iU1zMIoEP%2FopHqv%2FtZ7%2FK5X7%2BII%3D\n",
            "Resolving dwstor01.blob.core.windows.net (dwstor01.blob.core.windows.net)... 52.239.148.4\n",
            "Connecting to dwstor01.blob.core.windows.net (dwstor01.blob.core.windows.net)|52.239.148.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 The specified resource does not exist.\n",
            "2022-01-03 03:42:05 ERROR 404: The specified resource does not exist..\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 데이터셋 및 설정 파일 다운로드\n",
        "!wget -O fruit_dataset.zip \"https://dwstor01.blob.core.windows.net/snue/fruit_dataset.zip?sp=r&st=2021-12-29T02:54:03Z&s[…]mRXtTvX6mydEyb9iU1zMIoEP%2FopHqv%2FtZ7%2FK5X7%2BII%3D\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oRA-Em_LB_kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xbb4h_xKJkX",
        "outputId": "02f9d04e-f563-46ad-be4a-11c1bae4127a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Jan  3 03:42 .\n",
            "drwxr-xr-x 1 root root 4096 Jan  3 03:30 ..\n",
            "-rw-r--r-- 1 root root    0 Jan  3 03:42 fruit_dataset.zip\n"
          ]
        }
      ],
      "source": [
        "# 다운로드 파일 체크\n",
        "!ls -al"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THhnes1ckVA5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "042f970c-00af-4557-bf3c-6ab851115de6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-963beb685b9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 압축 해제\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fruit_dataset.zip'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "from shutil import copy\n",
        "\n",
        "fileName = 'fruit_dataset.zip' \n",
        "\n",
        "# 압축 해제\n",
        "ds = ZipFile(fileName)\n",
        "ds.extractall()\n",
        "\n",
        "# os.remove(fileName)\n",
        "print('Extracted zip file ' + fileName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wkn9zi50oxw"
      },
      "outputs": [],
      "source": [
        "# 압축 파일 해제 확인\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks5_44aqPtVk"
      },
      "outputs": [],
      "source": [
        "# 작업 디렉토리 \n",
        "%cd fruit_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yre-rvSJ83bW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Pascal VOC 데이터셋을 TF Record로 변환하기 위한 중간 과정\n",
        "def xml_to_csv(path):\n",
        "    xml_list = []\n",
        "    for xml_file in glob.glob(path + '/*.xml'):\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        for member in root.findall('object'):\n",
        "            value = (root.find('filename').text,\n",
        "                     int(root.find('size')[0].text),\n",
        "                     int(root.find('size')[1].text),\n",
        "                     member[0].text,\n",
        "                     int(member[4][0].text),\n",
        "                     int(member[4][1].text),\n",
        "                     int(member[4][2].text),\n",
        "                     int(member[4][3].text)\n",
        "                     )\n",
        "            xml_list.append(value)\n",
        "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
        "    return xml_df\n",
        "\n",
        "\n",
        "def main():\n",
        "\tfor directory in ['train','test']:\n",
        "\t\timage_path = os.path.join(os.getcwd(), 'images/{}'.format(directory))\n",
        "\t\txml_df = xml_to_csv(image_path)\n",
        "\t\txml_df.to_csv('data/{}_labels.csv'.format(directory), index=None)\n",
        "\t\tprint('Successfully converted xml to csv.')\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXeqgzNhluaC"
      },
      "outputs": [],
      "source": [
        "# 작업 경로 확인\n",
        "%cd /content/dataset/fruit_dataset\n",
        "\n",
        "# TF Record 생성\n",
        "!python generate_tfrecord.py --csv_input=data/train_labels.csv  --output_path=data/train.record --image_dir=images/train\n",
        "!python generate_tfrecord.py --csv_input=data/test_labels.csv  --output_path=data/test.record --image_dir=images/test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeyO_oSKdhsG"
      },
      "source": [
        "# pretrained model 다운로드\n",
        "\n",
        "-  **faster rcnn inception v2 coco 2018 01 28** 모델 다운로드 - checkpoint로 사용\n",
        "- [Model Zoo 참조](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZJly9QBTS1u"
      },
      "outputs": [],
      "source": [
        "# models 작업 준비\n",
        "%cd /content/dataset\n",
        "\n",
        "# 기존 작업이 있을 경우 삭제\n",
        "%rm -rf models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUDk1gLQsWOz"
      },
      "outputs": [],
      "source": [
        "%mkdir /content/dataset/models\n",
        "%cd /content/dataset/models\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "MODEL = 'faster_rcnn_inception_v2_coco_2018_01_28'\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DEST_DIR = 'faster_rcnn_inception_v2'\n",
        "\n",
        "with urllib.request.urlopen(DOWNLOAD_BASE+MODEL_FILE) as response, open(MODEL_FILE, 'wb') as out_file:\n",
        "  shutil.copyfileobj(response, out_file)\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "  shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBE3oTN8kwAv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAYXLhS2uZ9X"
      },
      "source": [
        "# 모델 트레이닝\n",
        "\n",
        "- 트레이닝 디렉토리 및 config 값 체크"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkGtiNzE5qsm"
      },
      "outputs": [],
      "source": [
        "# 파일 path 체크\n",
        "!ls /content/models/research/object_detection/legacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOyM7iND6DZj"
      },
      "outputs": [],
      "source": [
        "# config 파일 체크\n",
        "!ls /content/dataset/fruit_dataset/data/faster_rcnn_inception_v2_fruits.config\n",
        "#!cat /content/dataset/fruit_dataset/data/faster_rcnn_inception_v2_fruits.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDv2gHC0ocbL"
      },
      "outputs": [],
      "source": [
        "# tensorboard 준비\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aFmHc6wcR0Y"
      },
      "outputs": [],
      "source": [
        "%mkdir inference_graph\n",
        "%cd /content/models/research/object_detection/legacy\n",
        "\n",
        "# training 수행 subprocess로 수행할까?\n",
        "!python train.py --logtostderr --train_dir=training/ --pipeline_config_path=/content/dataset/fruit_dataset/data/faster_rcnn_inception_v2_fruits.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiVNPBkrAHh1"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir=training\n",
        "# training 중에 blocking되어 중지. Training을 재시작하면 변경"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjJCB5NKK4Nb"
      },
      "source": [
        "# 트레이닝 완료 모델을 export \n",
        "\n",
        "- `export_inference_graph`를 수행해 export 수행. checkpoint 숫자로 설정 주의."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swEafA2BEIR5"
      },
      "outputs": [],
      "source": [
        "%cd legacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icFMevZaXPph"
      },
      "outputs": [],
      "source": [
        "# 파일리스트 체크\n",
        "!ls training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfNlbs1R-B2q"
      },
      "outputs": [],
      "source": [
        "%cd /content/models/research/object_detection\n",
        "# Step 카운트 파라미터 변경 후 수행\n",
        "!python export_inference_graph.py --input_type image_tensor --pipeline_config_path /content/dataset/fruit_dataset/data/faster_rcnn_inception_v2_fruits.config --trained_checkpoint_prefix /content/models/research/object_detection/legacy/training/model.ckpt-1268 --output_directory inference_graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYyaQyyKxaDk"
      },
      "source": [
        "# Object detection inference 수행\n",
        "- 파일을 Colab으로 업로드\n",
        "- Inference 수행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFqRK1TBYl30"
      },
      "outputs": [],
      "source": [
        "# Object detection inference 수행할 파라미터 정리 \n",
        "uploaded = None\n",
        "image_path = None\n",
        "TEST_IMAGE_PATHS = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI8Ya_6GE9ll"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "from google.colab import files\n",
        "from os import path\n",
        "\n",
        "uploaded = files.upload()\n",
        "# Colab에서 이미지 파일 업로드. \".jpg\" 포맷의 테스트할 이미지를 업로드 가능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyzrXOafFJnC"
      },
      "outputs": [],
      "source": [
        "# Colab에 업로드한 스트림을 파일로 저장\n",
        "%cd /content\n",
        "for name, data in uploaded.items():\n",
        "  with open(name, 'wb') as f:\n",
        "    f.write(data)\n",
        "    f.close()\n",
        "    print('saved file ' + name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnpRztAAFd1R"
      },
      "outputs": [],
      "source": [
        "# export한 모델 체크\n",
        "!ls /content/models/research/object_detection/inference_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-b4D_L-FmqJ"
      },
      "outputs": [],
      "source": [
        "# 업로드한 이미지 파일들 체크\n",
        "!ls /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUy6KXMToLVc"
      },
      "outputs": [],
      "source": [
        "# inference 및 notebook에 출력 수행\n",
        "%cd /content/models/research/object_detection\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "import glob\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import PIL.ImageFont as ImageFont\n",
        "\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "#if tf.__version__ < '1.4.0':\n",
        "#  raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\n",
        "  \n",
        "# This is needed to display the images.\n",
        "%matplotlib inline\n",
        "\n",
        "from utils import label_map_util\n",
        "from utils import visualization_utils as vis_util\n",
        "\n",
        "\n",
        "# What model to download.\n",
        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
        "PATH_TO_CKPT = '/content/models/research/object_detection/inference_graph' + '/frozen_inference_graph.pb'\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = os.path.join('/content/dataset/fruit_dataset/data', 'object-detection.pbtxt')\n",
        "\n",
        "NUM_CLASSES = 3  # 현재 Fruit dataset은 3개의 class 사용\n",
        "\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "  od_graph_def = tf.GraphDef()\n",
        "  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "    serialized_graph = fid.read()\n",
        "    od_graph_def.ParseFromString(serialized_graph)\n",
        "    tf.import_graph_def(od_graph_def, name='')\n",
        "     \n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "def load_image_into_numpy_array(image):\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
        "PATH_TO_TEST_IMAGES_DIR = '/content'\n",
        "TEST_IMAGE_PATHS = [f for f in glob.glob('/content/*.jpg')]  # JPG 파일 포맷 주의\n",
        "print('TEST_IMAGE_PATHS: ', TEST_IMAGE_PATHS)\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur0oBeiww6Z_"
      },
      "outputs": [],
      "source": [
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (20, 16)  # plot 출력 이미지 크기\n",
        "\n",
        "def run_inference_for_single_image(image, graph):\n",
        "  with graph.as_default():\n",
        "    with tf.Session() as sess:\n",
        "      # Get handles to input and output tensors\n",
        "      ops = tf.get_default_graph().get_operations()\n",
        "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
        "      tensor_dict = {}\n",
        "      for key in [\n",
        "          'num_detections', 'detection_boxes', 'detection_scores',\n",
        "          'detection_classes', 'detection_masks'\n",
        "      ]:\n",
        "        tensor_name = key + ':0'\n",
        "        if tensor_name in all_tensor_names:\n",
        "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "              tensor_name)\n",
        "      if 'detection_masks' in tensor_dict:\n",
        "        # The following processing is only for single image\n",
        "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
        "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
        "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
        "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
        "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
        "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "        detection_masks_reframed = tf.cast(\n",
        "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "        # Follow the convention by adding back the batch dimension\n",
        "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "            detection_masks_reframed, 0)\n",
        "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "      # Run inference\n",
        "      output_dict = sess.run(tensor_dict,\n",
        "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "      # 출력은 numpy 포맷. 변경 필요 \n",
        "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
        "      output_dict['detection_classes'] = output_dict[\n",
        "          'detection_classes'][0].astype(np.uint8)\n",
        "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "      if 'detection_masks' in output_dict:\n",
        "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "  return output_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKgfEQq8w8-k"
      },
      "outputs": [],
      "source": [
        "for image_path in TEST_IMAGE_PATHS:\n",
        "  print('image_path:', image_path)\n",
        "  image = Image.open(image_path)\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  image_np = load_image_into_numpy_array(image)\n",
        "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "  # Visualization of the results of a detection.\n",
        "\n",
        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks'),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=8)\n",
        "  plt.figure(figsize=IMAGE_SIZE)\n",
        "  plt.imshow(image_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYxKuOmA-aMU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1xS7n2PzOWX"
      },
      "outputs": [],
      "source": [
        "# dataset CSV 파일 체크\n",
        "# %cd /content/dataset/fruit_dataset/data\n",
        "# !cat train_labels.csv"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of train_detect_own_dataset_tf_colab4.ipynb의 사본의 사본",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}